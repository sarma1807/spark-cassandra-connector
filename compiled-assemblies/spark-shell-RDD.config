// *** FOR Spark with RDDs METHOD ***

sc.stop
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import com.datastax.spark.connector._
import sys.process._

val conf = new SparkConf(true)
// provide multiple cassandra contact points from same data center
conf.set("spark.cassandra.connection.host", "192.168.1.151,192.168.1.152")
// need following for cassandra authentication
conf.set("spark.cassandra.auth.username", "thor")
conf.set("spark.cassandra.auth.password", "oracle")
// need following for cassandra timeouts - need high values to finish large jobs
conf.set("spark.cassandra.connection.keepAliveMS", "300000")
conf.set("spark.cassandra.connection.timeoutMS", "300000")
conf.set("spark.cassandra.read.timeoutMS", "300000")
conf.set("spark.cassandra.input.split.sizeInMB", "512")
// consistency level : ALL / EACH_QUORUM / QUORUM / LOCAL_QUORUM / ONE / TWO / THREE / LOCAL_ONE / ANY
conf.set("spark.cassandra.input.consistency.level", "LOCAL_QUORUM")
conf.set("spark.cassandra.output.consistency.level", "LOCAL_QUORUM")

// finally build new sparkContext using above conf
val sc = new SparkContext("spark://192.168.1.173:7077", "SparkCassConnectApp", conf)

// print list of included jars + context conf settings
sc.jars.foreach(println)
spark.sparkContext.listJars.foreach(println)
sc.getConf.getAll.foreach(println)
// spark.conf.getAll.foreach(println)
"date".!

